{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: si_prefix in /home/arijit/PycharmProjects/Schule/schule/lib/python3.8/site-packages (1.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install si_prefix\n",
    "!pip install pykeops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# K-Nearest Neighbours search (WIP)\n",
    "\n",
    "Let's compare the performances of PyTorch, JAX, FAISS and KeOps fpr \n",
    "K-NN queries on random samples and standard datasets.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>In this demo, we use exact **bruteforce** computations \n",
    "    (tensorized for PyTorch and online for KeOps), without leveraging any multiscale\n",
    "    or low-rank (Nystroem/multipole) decomposition of the Kernel matrix.\n",
    "    First support for these approximation schemes is scheduled for\n",
    "    May-June 2021.</p></div>\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'si_prefix'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-3-a8426e827d1f>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mfunctools\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpartial\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m from benchmark_utils import (\n\u001B[0m\u001B[1;32m      7\u001B[0m     \u001B[0mflatten\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mrandom_normal\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Schule/moco/benchmark_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mmpl\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mmatplotlib\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 13\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0msi_prefix\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0msi_format\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'si_prefix'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "from benchmark_utils import (\n",
    "    flatten,\n",
    "    random_normal,\n",
    "    full_benchmark,\n",
    "    timer,\n",
    "    tensor,\n",
    "    int_tensor,\n",
    "    jax_tensor,\n",
    ")\n",
    "from dataset_utils import generate_samples\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark specifications:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Values of K that we'll loop upon:\n",
    "Ks = [1, 2, 5, 10, 20, 50, 100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple bruteforce implementations\n",
    "\n",
    "Define a simple Gaussian RBF product, using a **tensorized** implementation.\n",
    "Note that expanding the squared norm $\\|x-y\\|^2$ as a sum\n",
    "$\\|x\\|^2 - 2 \\langle x, y \\rangle + \\|y\\|^2$ allows us\n",
    "to leverage the fast matrix-matrix product of the BLAS/cuBLAS\n",
    "libraries.\n",
    "\n",
    "\n",
    "PyTorch bruteforce:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def KNN_KeOps(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "        # Setup the K-NN estimator:\n",
    "        x_train = tensor(x_train)\n",
    "        start = timer()\n",
    "\n",
    "        # N.B.: The \"training\" time here should be negligible.\n",
    "        elapsed = timer() - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = tensor(x_test)\n",
    "            start = timer()\n",
    "\n",
    "            # Actual K-NN query:\n",
    "\n",
    "            elapsed = timer() - start\n",
    "\n",
    "            indices = indices.cpu().numpy()\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def KNN_torch(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "        # Setup the K-NN estimator:\n",
    "        x_train = tensor(x_train)\n",
    "        start = timer()\n",
    "        # The \"training\" time here should be negligible:\n",
    "        x_train_norm = (x_train ** 2).sum(-1)\n",
    "        elapsed = timer() - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = tensor(x_test)\n",
    "            start = timer()\n",
    "\n",
    "            # Actual K-NN query:\n",
    "            if metric == \"euclidean\":\n",
    "                x_test_norm = (x_test ** 2).sum(-1)\n",
    "                diss = (\n",
    "                    x_test_norm.view(-1, 1)\n",
    "                    + x_train_norm.view(1, -1)\n",
    "                    - 2 * x_test @ x_train.t()\n",
    "                )\n",
    "\n",
    "            elif metric == \"manhattan\":\n",
    "                diss = (x_test[:, None, :] - x_train[None, :, :]).abs().sum(dim=2)\n",
    "\n",
    "            elif metric == \"angular\":\n",
    "                diss = -x_test @ x_train.t()\n",
    "\n",
    "            elif metric == \"hyperbolic\":\n",
    "                x_test_norm = (x_test ** 2).sum(-1)\n",
    "                diss = (\n",
    "                    x_test_norm.view(-1, 1)\n",
    "                    + x_train_norm.view(1, -1)\n",
    "                    - 2 * x_test @ x_train.t()\n",
    "                )\n",
    "                diss /= x_test[:, 0].view(-1, 1) * x_train[:, 0].view(1, -1)\n",
    "\n",
    "            out = diss.topk(K, dim=1, largest=False)\n",
    "\n",
    "            elapsed = timer() - start\n",
    "            indices = out.indices.cpu().numpy()\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch bruteforce, with small batches to avoid memory overflows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def KNN_torch_batch_loop(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "        # Setup the K-NN estimator:\n",
    "        x_train = tensor(x_train)\n",
    "        Ntrain, D = x_train.shape\n",
    "        start = timer()\n",
    "        # The \"training\" time here should be negligible:\n",
    "        x_train_norm = (x_train ** 2).sum(-1)\n",
    "        elapsed = timer() - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = tensor(x_test)\n",
    "\n",
    "            # Estimate the largest reasonable batch size:\n",
    "            Ntest = x_test.shape[0]\n",
    "            #  torch.cuda.get_device_properties(deviceId).total_memory\n",
    "            av_mem = int(5e8)\n",
    "            Ntest_loop = min(max(1, av_mem // (4 * D * Ntrain)), Ntest)\n",
    "            Nloop = (Ntest - 1) // Ntest_loop + 1\n",
    "            # print(f\"{Ntest} queries, split in {Nloop} batches of {Ntest_loop} queries each.\")\n",
    "            out = int_tensor(Ntest, K)\n",
    "\n",
    "            start = timer()\n",
    "            # Actual K-NN query:\n",
    "            for k in range(Nloop):\n",
    "                x_test_k = x_test[Ntest_loop * k : Ntest_loop * (k + 1), :]\n",
    "                if metric == \"euclidean\":\n",
    "                    x_test_norm = (x_test_k ** 2).sum(-1)\n",
    "                    diss = (\n",
    "                        x_test_norm.view(-1, 1)\n",
    "                        + x_train_norm.view(1, -1)\n",
    "                        - 2 * x_test_k @ x_train.t()\n",
    "                    )\n",
    "\n",
    "                elif metric == \"manhattan\":\n",
    "                    diss = (x_test_k[:, None, :] - x_train[None, :, :]).abs().sum(dim=2)\n",
    "\n",
    "                elif metric == \"angular\":\n",
    "                    diss = -x_test_k @ x_train.t()\n",
    "\n",
    "                elif metric == \"hyperbolic\":\n",
    "                    x_test_norm = (x_test_k ** 2).sum(-1)\n",
    "                    diss = (\n",
    "                        x_test_norm.view(-1, 1)\n",
    "                        + x_train_norm.view(1, -1)\n",
    "                        - 2 * x_test_k @ x_train.t()\n",
    "                    )\n",
    "                    diss /= x_test_k[:, 0].view(-1, 1) * x_train[:, 0].view(1, -1)\n",
    "\n",
    "                out[Ntest_loop * k : Ntest_loop * (k + 1), :] = diss.topk(\n",
    "                    K, dim=1, largest=False\n",
    "                ).indices\n",
    "                del diss\n",
    "            # torch.cuda.empty_cache()\n",
    "\n",
    "            elapsed = timer() - start\n",
    "            indices = out.cpu().numpy()\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance matrices with JAX:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2, 3))\n",
    "def knn_jax_fun(x_train, x_test, K, metric):\n",
    "    if metric == \"euclidean\":\n",
    "        diss = (\n",
    "            (x_test ** 2).sum(-1)[:, None]\n",
    "            + (x_train ** 2).sum(-1)[None, :]\n",
    "            - 2 * x_test @ x_train.T\n",
    "        )\n",
    "    elif metric == \"manhattan\":\n",
    "        diss = jax.lax.abs(x_test[:, None, :] - x_train[None, :, :]).sum(-1)\n",
    "    elif metric == \"angular\":\n",
    "        diss = -x_test @ x_train.T\n",
    "    elif metric == \"hyperbolic\":\n",
    "        diss = (\n",
    "            (x_test ** 2).sum(-1)[:, None]\n",
    "            + (x_train ** 2).sum(-1)[None, :]\n",
    "            - 2 * x_test @ x_train.T\n",
    "        )\n",
    "        diss = diss / (x_test[:, 0][:, None] * x_train[:, 0][None, :])\n",
    "\n",
    "    indices = jax.lax.top_k(-diss, K)[1]\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX bruteforce:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def KNN_JAX(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "\n",
    "        # Setup the K-NN estimator:\n",
    "        start = timer(use_torch=False)\n",
    "        x_train = jax_tensor(x_train)\n",
    "        elapsed = timer(use_torch=False) - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = jax_tensor(x_test)\n",
    "\n",
    "            # Actual K-NN query:\n",
    "            start = timer(use_torch=False)\n",
    "            indices = knn_jax_fun(x_train, x_test, K, metric)\n",
    "            indices = np.array(indices)\n",
    "            elapsed = timer(use_torch=False) - start\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JAX bruteforce, with small batches to avoid memory overflows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def KNN_JAX_batch_loop(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "\n",
    "        # Setup the K-NN estimator:\n",
    "        start = timer(use_torch=False)\n",
    "        x_train = jax_tensor(x_train)\n",
    "        elapsed = timer(use_torch=False) - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = jax_tensor(x_test)\n",
    "\n",
    "            # Estimate the largest reasonable batch size\n",
    "            #  torch.cuda.get_device_properties(deviceId).total_memory\n",
    "            av_mem = int(5e8)\n",
    "            Ntrain, D = x_train.shape\n",
    "            Ntest = x_test.shape[0]\n",
    "            Ntest_loop = min(max(1, av_mem // (4 * D * Ntrain)), Ntest)\n",
    "            Nloop = (Ntest - 1) // Ntest_loop + 1\n",
    "            # print(f\"{Ntest} queries, split in {Nloop} batches of {Ntest_loop} queries each.\")\n",
    "            indices = np.zeros((Ntest, K), dtype=int)\n",
    "\n",
    "            start = timer(use_torch=False)\n",
    "            # Actual K-NN query:\n",
    "            for k in range(Nloop):\n",
    "                x_test_k = x_test[Ntest_loop * k : Ntest_loop * (k + 1), :]\n",
    "                indices[Ntest_loop * k : Ntest_loop * (k + 1), :] = knn_jax_fun(\n",
    "                    x_train, x_test_k, K, metric\n",
    "                )\n",
    "            elapsed = timer(use_torch=False) - start\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeOps bruteforce implementation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pykeops'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-249b598afcb3>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpykeops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtorch\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mLazyTensor\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mVi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mVj\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mKNN_KeOps\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mK\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetric\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"euclidean\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'pykeops'"
     ]
    }
   ],
   "source": [
    "from pykeops.torch import LazyTensor, Vi, Vj\n",
    "\n",
    "\n",
    "def KNN_KeOps(K, metric=\"euclidean\", **kwargs):\n",
    "    def fit(x_train):\n",
    "        # Setup the K-NN estimator:\n",
    "        x_train = tensor(x_train)\n",
    "        start = timer()\n",
    "\n",
    "        # Encoding as KeOps LazyTensors:\n",
    "        D = x_train.shape[1]\n",
    "        X_i = Vi(0, D)\n",
    "        X_j = Vj(1, D)\n",
    "\n",
    "        # Symbolic distance matrix:\n",
    "        if metric == \"euclidean\":\n",
    "            D_ij = ((X_i - X_j) ** 2).sum(-1)\n",
    "        elif metric == \"manhattan\":\n",
    "            D_ij = (X_i - X_j).abs().sum(-1)\n",
    "        elif metric == \"angular\":\n",
    "            D_ij = -(X_i | X_j)\n",
    "        elif metric == \"hyperbolic\":\n",
    "            D_ij = ((X_i - X_j) ** 2).sum(-1) / (X_i[0] * X_j[0])\n",
    "\n",
    "        # K-NN query operator:\n",
    "        KNN_fun = D_ij.argKmin(K, dim=1)\n",
    "\n",
    "        # N.B.: The \"training\" time here should be negligible.\n",
    "        elapsed = timer() - start\n",
    "\n",
    "        def f(x_test):\n",
    "            x_test = tensor(x_test)\n",
    "            start = timer()\n",
    "\n",
    "            # Actual K-NN query:\n",
    "            indices = KNN_fun(x_test, x_train)\n",
    "\n",
    "            elapsed = timer() - start\n",
    "\n",
    "            indices = indices.cpu().numpy()\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciKit-Learn tree-based and bruteforce methods\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "def KNN_sklearn(K, metric=\"euclidean\", algorithm=None, **kwargs):\n",
    "\n",
    "    if metric in [\"euclidean\", \"angular\"]:\n",
    "        p = 2\n",
    "    elif metric == \"manhattan\":\n",
    "        p = 1\n",
    "    else:\n",
    "        raise NotImplementedError(\"This distance is not supported.\")\n",
    "\n",
    "    KNN_meth = NearestNeighbors(n_neighbors=K, algorithm=algorithm, p=p, n_jobs=-1)\n",
    "\n",
    "    def fit(x_train):\n",
    "        # Setup the K-NN estimator:\n",
    "        start = timer()\n",
    "        KNN_fun = KNN_meth.fit(x_train).kneighbors\n",
    "        elapsed = timer() - start\n",
    "\n",
    "        def f(x_test):\n",
    "            start = timer()\n",
    "            distances, indices = KNN_fun(x_test)\n",
    "            elapsed = timer() - start\n",
    "\n",
    "            return indices, elapsed\n",
    "\n",
    "        return f, elapsed\n",
    "\n",
    "    return fit\n",
    "\n",
    "\n",
    "KNN_sklearn_auto = partial(KNN_sklearn, algorithm=\"auto\")\n",
    "KNN_sklearn_ball_tree = partial(KNN_sklearn, algorithm=\"ball_tree\")\n",
    "KNN_sklearn_kd_tree = partial(KNN_sklearn, algorithm=\"kd_tree\")\n",
    "KNN_sklearn_brute = partial(KNN_sklearn, algorithm=\"brute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy vs. PyTorch vs. KeOps (Gpu)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def run_KNN_benchmark(name):\n",
    "\n",
    "    # Load the dataset and some info:\n",
    "    dataset = generate_samples(name)(1)\n",
    "    N_train, dimension = dataset[\"train\"].shape\n",
    "    N_test, _ = dataset[\"test\"].shape\n",
    "    metric = dataset[\"metric\"]\n",
    "\n",
    "    # Routines to benchmark:\n",
    "    routines = [\n",
    "        (KNN_sklearn_auto, \"sklearn, auto (CPU)\", {}),\n",
    "        (KNN_sklearn_ball_tree, \"sklearn, Ball-tree (CPU)\", {}),\n",
    "        (KNN_sklearn_kd_tree, \"sklearn, KD-tree (CPU)\", {}),\n",
    "        (KNN_sklearn_brute, \"sklearn, bruteforce (CPU)\", {}),\n",
    "        (KNN_torch, \"PyTorch (GPU)\", {}),\n",
    "        (KNN_torch_batch_loop, \"PyTorch (small batches, GPU)\", {}),\n",
    "        (KNN_KeOps, \"KeOps (GPU)\", {}),\n",
    "        (KNN_JAX, \"JAX (GPU)\", {}),\n",
    "        (KNN_JAX_batch_loop, \"JAX (small batches, GPU)\", {}),\n",
    "    ]\n",
    "\n",
    "    # Actual run:\n",
    "    full_benchmark(\n",
    "        f\"K-NN search on {name}: {N_test:,} queries on a dataset of {N_train:,} points\\nin dimension {dimension:,} with a {metric} metric.\",\n",
    "        routines,\n",
    "        generate_samples(name),\n",
    "        min_time=1e-4,\n",
    "        max_time=10,\n",
    "        problem_sizes=Ks,\n",
    "        xlabel=\"Number of neighbours K\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On random samples:\n",
    "\n",
    "Small dataset in $\\mathbb{R}^3$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "run_KNN_benchmark(\"R^D a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large dataset in $\\mathbb{R}^3$:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "run_KNN_benchmark(\"R^D b\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}